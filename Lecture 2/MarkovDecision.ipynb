{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "Markov decision processes are just Markov reward processes with actions thrown in. \n",
    "\n",
    "<img src=\"/files/images/mdp_definition.png\", width=500px>\n",
    "\n",
    "With the introduction of actions the rewards and transition probabilities are now conditioned on which action was taken. In the case of our simple Student MDP, most of the actions-save for the \"Pub\" action-are deterministic. The Pub action randomly deposits the actor into one of the 3 \"Class\" states.\n",
    "\n",
    "<img src=\"/files/images/student_mdp.png\", width=500px>\n",
    "\n",
    "## Policies\n",
    "\n",
    "No longer being at the mercy of fate, we now need to be able to choose actions depending on which state we're in. That's the job of the *Policy*. Policies are funcitons which map states to action probabilities. Below we'll consider a policy that gives each available action an equal probability of being chosen. \n",
    "\n",
    "<img src=\"/files/images/policy.png\", width=500px>\n",
    "\n",
    "## Generating samples\n",
    "\n",
    "With a properly defined set of transition probabilities-one for each action-and a policy which probabilistically chooses actions in each state, we can sample trajectories just like in MRPs and MCs.\n",
    "\n",
    "1. The agent starts in a state \n",
    "2. The policy determines which action will be chosen (probabilistically).\n",
    "3. A reward is given for the resulting action.\n",
    "4. The associated transition probabilities for that action determine where the agent will end up next. \n",
    "5. Repeat until the terminal state is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_names = [\"C1\", \"C2\", \"C3\", \"FB\", \"Sleep\"]\n",
    "\n",
    "\n",
    "# Each action has an associated transition matrix\n",
    "# Most transitions are deterministic with a value of 1\n",
    "# with the exception of the \"Pub\" action, which can only be taken from\n",
    "# \"C3\" (third row), and transitions the agent to C1, C2 or C3 randomly\n",
    "_transitions = {\"Study\": [[0, 1, 0, 0, 0],\n",
    "                           [0, 0, 1, 0, 0],\n",
    "                           [0, 0, 0, 0, 1],\n",
    "                           [0, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 0, 0]],\n",
    "                \"Sleep\": [[0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1],\n",
    "                          [0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1]],\n",
    "                \"FB\": [[0, 0, 0, 1, 0],\n",
    "                       [0, 0, 0, 0, 0],\n",
    "                       [0, 0, 0, 0, 0],\n",
    "                       [0, 0, 0, 1, 0],\n",
    "                       [0, 0, 0, 0, 0]],\n",
    "                \"Quit\": [[0, 0, 0, 0, 0],\n",
    "                         [0, 0, 0, 0, 0],\n",
    "                         [0, 0, 0, 0, 0],\n",
    "                         [1, 0, 0, 0, 0],\n",
    "                         [0, 0, 0, 0, 0]],\n",
    "                \"Pub\": [[0, 0, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0],\n",
    "                        [.2, .4, .4, 0, 0],\n",
    "                        [0, 0, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0]]}\n",
    "\n",
    "\n",
    "# Each entry for an action corresponds to the state it is taken in.\n",
    "# Choosing to \"Study\" while in \"C3\" yields a +10 reward.\n",
    "# \"Study\" will never take an agent to \"FB\" or \"Sleep\", so it has a None value in those states\n",
    "_rewards = {\n",
    "    \"Study\": [-2, -2, 10, None, None],\n",
    "    \"Sleep\": [None, 0, None, None, 0],\n",
    "    \"FB\": [-1, None, None, -1, None],\n",
    "    \"Quit\": [None, None, None, 0, None],\n",
    "    \"Pub\": [None, None, 1, None, None]\n",
    "}\n",
    "\n",
    "\n",
    "# If the agent is in \"C1\" it has a 50% chance of choosing to either Study or go to FB.\n",
    "# Technically ALL actions should be represented in each state but with probability 0\n",
    "# but that's too much writing for no payoff so I didn't bother. Not sorry.\n",
    "_policy = {\n",
    "    \"C1\": {\"Study\": .5, \"FB\": .5},\n",
    "    \"C2\": {\"Study\": .5, \"Sleep\": .5},\n",
    "    \"C3\": {\"Study\": .5, \"Pub\": .5},\n",
    "    \"FB\": {\"Quit\": .5, \"FB\": .5},\n",
    "    \"Sleep\": {\"Sleep\": 1}\n",
    "}\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class MDP:\n",
    "    def __init__(self, transitions, rewards, policy, state_names):\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.state_names = state_names\n",
    "        self._policy = policy\n",
    "        self.terminal_state = \"Sleep\"\n",
    "        \n",
    "    def policy(self, state):\n",
    "        probabilities = self._policy[state]\n",
    "        action = choice(list(probabilities.keys()), p=list(probabilities.values()))\n",
    "        return action\n",
    "    \n",
    "    def act(self, state):\n",
    "        action = self.policy(state)\n",
    "        p_matrix = self.transitions[action]\n",
    "        P = p_matrix[self.state_names.index(state)]\n",
    "        next_state = choice(self.state_names, p=P)\n",
    "        return action, next_state\n",
    "    \n",
    "        \n",
    "    def sample(self, state):\n",
    "        states = []\n",
    "        actions = []\n",
    "        while state != self.terminal_state:\n",
    "            states.append(state)\n",
    "            action, next_state = self.act(state)\n",
    "            actions.append(action)\n",
    "            state = next_state\n",
    "        states.append(self.terminal_state)\n",
    "        return states, actions\n",
    "    \n",
    "    \n",
    "    def G(self, sample, gamma=1):\n",
    "        states, actions = sample\n",
    "        states.pop()\n",
    "        rewards = []\n",
    "        for i in range(len(states)):\n",
    "            reward_list = self.rewards[actions[i]]\n",
    "            state_index = self.state_names.index(states[i])\n",
    "            # Make sure we're taking an action we're allowed to in this state\n",
    "            assert reward_list[state_index] is not None\n",
    "            rewards.append(reward_list[state_index] * gamma**i)\n",
    "            \n",
    "        return np.sum(rewards)\n",
    "    \n",
    "    def pprint(self, sample, **kwargs):\n",
    "        ret = self.G(sample, **kwargs)\n",
    "        states, actions = sample\n",
    "        idx = 0\n",
    "        trajectory = \"\"\n",
    "        for state, action in zip(states, actions):\n",
    "            R = self.rewards[action][self.state_names.index(state)]\n",
    "            trajectory += \"{state}: {action}({reward}) --> \".format(state=state, action=action, reward=R)\n",
    "        \n",
    "        trajectory += \"END\\n\"\n",
    "        print(\"TRAJECTORY: \\n\")\n",
    "        print(trajectory)\n",
    "        print(\"Total Reward = {}\".format(ret))\n",
    "        print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Trajectories\n",
    "\n",
    "Below are some examples of random trajectories generated using the **uniform random policy** and the **transition dynamics** and **rewards** specified in the MDP graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Action(Reward) --> Transition \n",
      "\n",
      "\n",
      "TRAJECTORY: \n",
      "\n",
      "C1: Study(-2) --> C2: Sleep(0) --> END\n",
      "\n",
      "Total Reward = -2\n",
      "--------------------------------------------------\n",
      "\n",
      "TRAJECTORY: \n",
      "\n",
      "C1: FB(-1) --> FB: Quit(0) --> C1: FB(-1) --> FB: FB(-1) --> FB: FB(-1) --> FB: FB(-1) --> FB: Quit(0) --> C1: FB(-1) --> FB: FB(-1) --> FB: Quit(0) --> C1: FB(-1) --> FB: Quit(0) --> C1: Study(-2) --> C2: Study(-2) --> C3: Pub(1) --> C3: Study(10) --> END\n",
      "\n",
      "Total Reward = -1\n",
      "--------------------------------------------------\n",
      "\n",
      "TRAJECTORY: \n",
      "\n",
      "C1: FB(-1) --> FB: FB(-1) --> FB: FB(-1) --> FB: Quit(0) --> C1: Study(-2) --> C2: Study(-2) --> C3: Study(10) --> END\n",
      "\n",
      "Total Reward = 3\n",
      "--------------------------------------------------\n",
      "\n",
      "TRAJECTORY: \n",
      "\n",
      "C1: FB(-1) --> FB: Quit(0) --> C1: FB(-1) --> FB: FB(-1) --> FB: FB(-1) --> FB: FB(-1) --> FB: FB(-1) --> FB: Quit(0) --> C1: Study(-2) --> C2: Sleep(0) --> END\n",
      "\n",
      "Total Reward = -8\n",
      "--------------------------------------------------\n",
      "\n",
      "TRAJECTORY: \n",
      "\n",
      "C1: Study(-2) --> C2: Sleep(0) --> END\n",
      "\n",
      "Total Reward = -2\n",
      "--------------------------------------------------\n",
      "\n",
      "TRAJECTORY: \n",
      "\n",
      "C1: Study(-2) --> C2: Sleep(0) --> END\n",
      "\n",
      "Total Reward = -2\n",
      "--------------------------------------------------\n",
      "\n",
      "TRAJECTORY: \n",
      "\n",
      "C1: Study(-2) --> C2: Sleep(0) --> END\n",
      "\n",
      "Total Reward = -2\n",
      "--------------------------------------------------\n",
      "\n",
      "TRAJECTORY: \n",
      "\n",
      "C1: FB(-1) --> FB: FB(-1) --> FB: FB(-1) --> FB: Quit(0) --> C1: FB(-1) --> FB: FB(-1) --> FB: Quit(0) --> C1: Study(-2) --> C2: Sleep(0) --> END\n",
      "\n",
      "Total Reward = -7\n",
      "--------------------------------------------------\n",
      "\n",
      "TRAJECTORY: \n",
      "\n",
      "C1: Study(-2) --> C2: Study(-2) --> C3: Study(10) --> END\n",
      "\n",
      "Total Reward = 6\n",
      "--------------------------------------------------\n",
      "\n",
      "TRAJECTORY: \n",
      "\n",
      "C1: FB(-1) --> FB: Quit(0) --> C1: FB(-1) --> FB: FB(-1) --> FB: FB(-1) --> FB: FB(-1) --> FB: Quit(0) --> C1: FB(-1) --> FB: FB(-1) --> FB: Quit(0) --> C1: FB(-1) --> FB: FB(-1) --> FB: FB(-1) --> FB: Quit(0) --> C1: Study(-2) --> C2: Study(-2) --> C3: Pub(1) --> C3: Pub(1) --> C2: Sleep(0) --> END\n",
      "\n",
      "Total Reward = -12\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mdp = MDP(_transitions, _rewards, _policy, state_names)\n",
    "print(\"State: Action(Reward) --> Transition \\n\\n\")\n",
    "for i in range(10):\n",
    "    sample = mdp.sample(\"C1\")\n",
    "    mdp.pprint(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Return under a specified Policy\n",
    "\n",
    "Just like with Markov Reward Processes, we can ask what the average or *expected* return would be. \n",
    "\n",
    "## State Value Function\n",
    "\n",
    "The expected return for a specific state is also known as the **state-value**. Unlike the MRP case, the state value in an MDP is necessarily conditioned on what policy the agent follows.\n",
    "\n",
    "<img src=\"/files/images/state_value_mdp.png\", width=500px>\n",
    "\n",
    "We can *estimate* the state-value by simply taking many samples and averaging over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C1': -1.1519999999999999,\n",
       " 'C2': 2.7252999999999998,\n",
       " 'C3': 7.3715000000000002,\n",
       " 'FB': -2.3479999999999999,\n",
       " 'Sleep': 0.0}"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Monte-Carlo State-Value Estimation\n",
    "estimates = {}\n",
    "for state in mdp.state_names:\n",
    "    rewards = []\n",
    "    for i in range(0, 10000):\n",
    "        sample = mdp.sample(state)\n",
    "        reward = mdp.G(sample)\n",
    "        rewards.append(reward)\n",
    "    estimates[state] = np.mean(rewards)\n",
    "estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Value Function\n",
    "\n",
    "We can also ask what the expected value of a particular *action* is under a specific policy. This is known as the **action-value**.\n",
    "\n",
    "<img src=\"/files/images/action_value_mdp.png\", width=500px>\n",
    "\n",
    "Just like with the state-value, we can *estimate* the action-value by averaging over many samples.\n",
    "\n",
    "In the (messy) example below, each state-action pair may not have the same number of samples associated with them, but each are guaranteed to have at least `min_iterations` informing their estimates. Nevertheless, we still get a rough estimate of the action-values under our uniform-random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C1': {'FB': -3.3879612038796121, 'Study': 0.64785239559567498},\n",
       " 'C2': {'Sleep': 0.0, 'Study': 5.3791620837916208},\n",
       " 'C3': {'Pub': 4.7533246675332466, 'Study': 10.0},\n",
       " 'FB': {'FB': -3.2624407582938391, 'Quit': -1.3082691730826916},\n",
       " 'Sleep': {'Sleep': []}}"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monte-Carlo Action-Value Estimation\n",
    "\n",
    "action_estimates = {\"C1\": {}, \"C2\": {}, \"C3\": {}, \"FB\": {}}\n",
    "\n",
    "action_rewards = {\n",
    "    \"C1\": {\"Study\": [], \"FB\": []},\n",
    "    \"C2\": {\"Study\": [], \"Sleep\": []},\n",
    "    \"C3\": {\"Study\": [], \"Pub\": []},\n",
    "    \"FB\": {\"Quit\": [], \"FB\": []},\n",
    "    \"Sleep\": {\"Sleep\": []}\n",
    "}\n",
    "\n",
    "min_iterations = 10000\n",
    "\n",
    "# Yeesh this is ugly\n",
    "for state in mdp.state_names:\n",
    "    if state == \"Sleep\":\n",
    "        continue\n",
    "        \n",
    "    possible_actions = list(action_rewards[state].keys())\n",
    "    while possible_actions:\n",
    "        sample = mdp.sample(state)\n",
    "        _, actions = sample\n",
    "        reward = mdp.G(sample)\n",
    "        action_rewards[state][actions[0]].append(reward)\n",
    "        \n",
    "        for action in possible_actions:\n",
    "            # Stop considering this action\n",
    "            # once all are popped, we're done with this state\n",
    "            if len(action_rewards[state][action]) >= min_iterations:\n",
    "                i = possible_actions.index(action)\n",
    "                possible_actions.pop(i)\n",
    "\n",
    "action_estimates = action_rewards\n",
    "for state in action_rewards:\n",
    "    if state == \"Sleep\":\n",
    "        continue\n",
    "    for action in action_rewards[state]:\n",
    "        action_estimates[state][action] = np.mean(action_rewards[state][action])\n",
    "action_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving an MDP Analytically\n",
    "\n",
    "We can get a closed-form solution for the state-value of our MDP under our uniform-random policy by converting it into an MRP and solving it the same way we did in the MRP notebook.\n",
    "\n",
    "To convert any MDP into an MRP, we simply average over the dynamics that our policy implies:\n",
    "\n",
    "<img src=\"/files/images/mdp_to_mrp_reward.png\", width=500px>\n",
    "<img src=\"/files/images/mdp_to_mrp_transition.png\", width=500px>\n",
    "\n",
    "Averaging over our policy like this converts our **action-dependent** rewards and transition probabilities into **action-independent** rewards and transition probabilities. With action-independent rewards and transitions we once again have an MRP, which we already know how to solve.\n",
    "\n",
    "Below is the student MDP under a uniform-random policy pre-converted to an MRP. How these numbers were found is left as an exercise to the reader. \n",
    "\n",
    "(I've always wanted to say that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "import numpy as np\n",
    "\n",
    "### Converting an MDP into an MRP\n",
    "\n",
    "state_names = [\"C1\", \"C2\", \"C3\", \"FB\", \"Sleep\"]\n",
    "\n",
    "# Probabilities changed to reflect uniform random policy\n",
    "\n",
    "# Notice Class 3 probabilities reflect possible pub choice:\n",
    "# Row 3 Column 1:\n",
    "# (.5 * .2) = .1 = probability of picking pub action (.5) AND\n",
    "# probability of being sent to class 1 (.2) as a result\n",
    "\n",
    "# Together they mean a .1 probability \n",
    "# of ending up back in C1 from C3\n",
    "\n",
    "p_matrix = [[0, .5, 0, .5, 0],\n",
    "            [0, 0, .5, 0, .5],\n",
    "            [.1, .2, .2, 0, .5],\n",
    "            [.5, 0, 0, .5, 0],\n",
    "            [0, 0, 0, 0, 0]]\n",
    "\n",
    "# Action rewards are weighted and summed by probability of being chosen\n",
    "# I.E: 5.5 = (.5 * 10) + (.5 * 1)\n",
    "_rewards = [-1.5, -1, 5.5, -.5, 0]\n",
    "\n",
    "\n",
    "gamma = 1\n",
    "R = np.array(_rewards)\n",
    "P = np.matrix(p_matrix)\n",
    "I = np.identity(len(p_matrix))\n",
    "\n",
    "solution = np.dot(np.linalg.inv((I-gamma*P)), R)\n",
    "solution = solution.tolist()[0]\n",
    "\n",
    "solutions = {}\n",
    "for state in range(len(state_names)):\n",
    "    solutions[state_names[state]] = solution[state]\n",
    "\n",
    "solutions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimates vs. Solution\n",
    "\n",
    "If we compare our estimates to the closed-form solution, it looks like we got pretty dang close. Playing around with how many samples were taken in each case might get us closer to the answer, so try playing around with the code yourself and see how close you can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values\n",
      "     Solution \t State-Value Estimate \t Action to State-Value Estimate\n",
      "     -------- \t -------------------- \t ------------------------------\n",
      "FB:   -2.308\t\t-2.348\t\t\t-2.285\n",
      "C3:   7.385\t\t7.372\t\t\t7.377\n",
      "C1:   -1.308\t\t-1.152\t\t\t-1.370\n",
      "C2:   2.692\t\t2.725\t\t\t2.690\n"
     ]
    }
   ],
   "source": [
    "print(\"State Values\")\n",
    "print(\"     Solution \\t State-Value Estimate \\t Action to State-Value Estimate\")\n",
    "print(\"     -------- \\t -------------------- \\t ------------------------------\")\n",
    "for state in solutions.keys():\n",
    "    if state == \"Sleep\":\n",
    "        continue\n",
    "    sol = solutions[state]\n",
    "    est = estimates[state]\n",
    "    \n",
    "    a_est = 0\n",
    "    # Averaging over all actions under the policy to get state-values\n",
    "    for action in action_estimates[state]:\n",
    "        a_est += mdp._policy[state][action] * action_estimates[state][action]\n",
    "    \n",
    "    print(\"{state}:   {sol:.3f}\\t\\t{est:.3f}\\t\\t\\t{a_est:.3f}\".format(state=state, sol=sol, est=est, a_est=a_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
