{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C1': -1.3076923076923084,\n",
       " 'C2': 2.6923076923076925,\n",
       " 'C3': 7.384615384615385,\n",
       " 'FB': -2.3076923076923075,\n",
       " 'Sleep': 0.0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Converting an MDP into an MRP\n",
    "\n",
    "state_names = [\"C1\", \"C2\", \"C3\", \"FB\", \"Sleep\"]\n",
    "\n",
    "# Probabilities changed to reflect uniform random policy\n",
    "\n",
    "# Notice Class 3 probabilities reflect possible pub choice:\n",
    "# Row 3 Column 1:\n",
    "# (.5 * .2) = .1 = probability of picking pub action (.5) AND\n",
    "# probability of being sent to class 1 (.2) as a result\n",
    "\n",
    "# Together they mean a .1 probability \n",
    "# of ending up back in C1 from C3\n",
    "\n",
    "p_matrix = [[0, .5, 0, .5, 0],\n",
    "            [0, 0, .5, 0, .5],\n",
    "            [.1, .2, .2, 0, .5],\n",
    "            [.5, 0, 0, .5, 0],\n",
    "            [0, 0, 0, 0, 0]]\n",
    "\n",
    "# Action rewards are weighted and summed by probability of being chosen\n",
    "# I.E: 5.5 = (.5 * 10) + (.5 * 1)\n",
    "_rewards = [-1.5, -1, 5.5, -.5, 0]\n",
    "\n",
    "\n",
    "gamma = 1\n",
    "R = np.array(_rewards)\n",
    "P = np.matrix(p_matrix)\n",
    "I = np.identity(len(p_matrix))\n",
    "\n",
    "solution = np.dot(np.linalg.inv((I-gamma*P)), R)\n",
    "solution = solution.tolist()[0]\n",
    "\n",
    "solutions = {}\n",
    "for state in range(len(state_names)):\n",
    "    solutions[state_names[state]] = solution[state]\n",
    "\n",
    "solutions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "state_names = [\"C1\", \"C2\", \"C3\", \"FB\", \"Sleep\"]\n",
    "\n",
    "_transitions = {\n",
    "    \"C1\": {\"Study\": [0, 1, 0, 0, 0], \"FB\": [0, 0, 0, 1, 0]},\n",
    "    \"C2\": {\"Study\": [0, 0, 1, 0, 0], \"Sleep\": [0, 0, 0, 0, 1]},\n",
    "    \"C3\": {\"Study\": [0, 0, 0, 0, 1], \"Pub\": [.2, .4, .4, 0, 0]},\n",
    "    \"FB\": {\"Quit\": [1, 0, 0, 0, 0], \"FB\": [0, 0, 0, 1, 0]},\n",
    "    \"Sleep\": {\"Sleep\": [0, 0, 0, 0, 1]}\n",
    "}\n",
    "\n",
    "_rewards = {\n",
    "    \"Study\": [-2, -2, 10, None, None],\n",
    "    \"Sleep\": [None, 0, None, None, 0],\n",
    "    \"FB\": [-1, None, None, -1, None],\n",
    "    \"Quit\": [None, None, None, 0, None],\n",
    "    \"Pub\": [None, None, 1, None, None]\n",
    "}\n",
    "\n",
    "_policy = {\n",
    "    \"C1\": {\"Study\": .5, \"FB\": .5},\n",
    "    \"C2\": {\"Study\": .5, \"Sleep\": .5},\n",
    "    \"C3\": {\"Study\": .5, \"Pub\": .5},\n",
    "    \"FB\": {\"Quit\": .5, \"FB\": .5},\n",
    "    \"Sleep\": {\"Sleep\": 1}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Just cus\n",
    "class ProbabilityMatrixException(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "    \n",
    "    \n",
    "class MDP:\n",
    "    def __init__(self, transitions, rewards, policy, state_names, terminal_state):\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.state_names = state_names\n",
    "        self.policy = policy\n",
    "        self.terminal_state = terminal_state\n",
    "        \n",
    "    def sample_policy(self, state):\n",
    "        probabilities = self.policy[state]\n",
    "        action = choice(list(probabilities.keys()), p=list(probabilities.values()))\n",
    "        return action\n",
    "    \n",
    "    def act(self, state, action):\n",
    "        possible_actions = self.transitions[state]\n",
    "        P = possible_actions[action]\n",
    "        next_state = choice(self.state_names, p=P)\n",
    "        return next_state\n",
    "        \n",
    "        \n",
    "    def sample(self, start):\n",
    "        states = []\n",
    "        actions = []\n",
    "        state = start\n",
    "        while state != self.terminal_state:\n",
    "            states.append(state)\n",
    "            action = self.sample_policy(state)\n",
    "            actions.append(action)\n",
    "            next_state = self.act(state, action)\n",
    "            state = next_state\n",
    "            \n",
    "        states.append(self.terminal_state)\n",
    "        return states, actions\n",
    "    \n",
    "    \n",
    "    def G(self, sample):\n",
    "        states, actions = sample\n",
    "        states.pop()\n",
    "        rewards = []\n",
    "        for i in range(len(states)):\n",
    "            reward_list = self.rewards[actions[i]]\n",
    "            state_index = self.state_names.index(states[i])\n",
    "            assert reward_list[state_index] is not None\n",
    "            rewards.append(reward_list[state_index])\n",
    "            \n",
    "        return np.sum(rewards)\n",
    "        \n",
    "\n",
    "mdp = MDP(_transitions, _rewards, _policy, state_names, \"Sleep\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C1': -1.2836000000000001,\n",
       " 'C2': 2.7210000000000001,\n",
       " 'C3': 7.319,\n",
       " 'FB': -2.3532000000000002,\n",
       " 'Sleep': 0.0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Monte-Carlo State-Value Estimation\n",
    "estimates = {}\n",
    "for state in mdp.state_names:\n",
    "    rewards = []\n",
    "    for i in range(0, 5000):\n",
    "        reward = mdp.G(mdp.sample(state))\n",
    "        rewards.append(reward)\n",
    "    estimates[state] = np.mean(rewards)\n",
    "estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lgendrot/anaconda/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/lgendrot/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C1': {'FB': -3.400354121581743, 'Study': 0.6522695460907818},\n",
       " 'C2': {'Sleep': 0.0, 'Study': 5.4152127033914921},\n",
       " 'C3': {'Pub': 4.7754449110177966, 'Study': 10.0},\n",
       " 'FB': {'FB': -3.3111243307555025, 'Quit': -1.4095180963807239},\n",
       " 'Sleep': {'Sleep': nan}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monte-Carlo Action-Value Estimation\n",
    "\n",
    "action_estimates = {\"C1\": {}, \"C2\": {}, \"C3\": {}, \"FB\": {}}\n",
    "\n",
    "action_rewards = {\n",
    "    \"C1\": {\"Study\": [], \"FB\": []},\n",
    "    \"C2\": {\"Study\": [], \"Sleep\": []},\n",
    "    \"C3\": {\"Study\": [], \"Pub\": []},\n",
    "    \"FB\": {\"Quit\": [], \"FB\": []},\n",
    "    \"Sleep\": {\"Sleep\": []}\n",
    "}\n",
    "\n",
    "min_iterations = 5000\n",
    "\n",
    "# Yeesh this is ugly\n",
    "for state in mdp.state_names:\n",
    "    if state == \"Sleep\":\n",
    "        continue\n",
    "        \n",
    "    possible_actions = list(action_rewards[state].keys())\n",
    "    while possible_actions:\n",
    "        sample = mdp.sample(state)\n",
    "        _, actions = sample\n",
    "        reward = mdp.G(sample)\n",
    "        action_rewards[state][actions[0]].append(reward)\n",
    "        \n",
    "        for action in possible_actions:\n",
    "            if len(action_rewards[state][action]) > min_iterations:\n",
    "                i = possible_actions.index(action)\n",
    "                possible_actions.pop(i)\n",
    "\n",
    "action_estimates = action_rewards\n",
    "for state in action_rewards:\n",
    "    for action in action_rewards[state]:\n",
    "        action_estimates[state][action] = np.mean(action_rewards[state][action])\n",
    "action_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
